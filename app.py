# ------------------------------------
# ‡πÑ‡∏ü‡∏•‡πå: app.py
# ------------------------------------
import gradio as gr
import cv2
from ultralytics import YOLO
from voice_guidance import speak_guidance, CLASS_NAME_MAP  # Import ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏û‡∏π‡∏î‡πÅ‡∏•‡∏∞ class names
import threading
import time
import os
from datetime import datetime
from pathlib import Path

# -------------------------------------------------------------------
# (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç!) ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Path ‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå best.pt ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏ó‡∏£‡∏ô‡πÑ‡∏î‡πâ
# -------------------------------------------------------------------
MODEL_PATH = 'artifacts/models/waste-sorter-best.pt' # ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà promote ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏≤‡∏Å DVC pipeline
# -------------------------------------------------------------------

# 1. ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• AI ‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß
try:
    print(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏≤‡∏Å: {MODEL_PATH}")
    model = YOLO(MODEL_PATH)
    print("‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
except Exception as e:
    print(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•: {e}")
    print("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Path ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà")
    exit()

# ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏™‡∏µ‡∏¢‡∏á
SPEECH_CONF_THRESHOLD = 0.4          # conf ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏û‡∏π‡∏î (‡∏•‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏û‡∏π‡∏î‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô)
SUSTAINED_FRAME_THRESHOLD = 2        # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ü‡∏£‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏û‡∏π‡∏î (‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î)
ANNOUNCE_COOLDOWN_SECONDS = 6        # ‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏π‡∏î‡∏ã‡πâ‡∏≥‡∏Ñ‡∏•‡∏≤‡∏™‡πÄ‡∏î‡∏¥‡∏°

# ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏†‡∏≤‡∏û
SAVE_IMAGES = True                   # ‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏†‡∏≤‡∏û
SAVE_CONF_THRESHOLD = 0.5            # conf ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏†‡∏≤‡∏û
SAVE_COOLDOWN_SECONDS = 3            # ‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏†‡∏≤‡∏û‡∏ã‡πâ‡∏≥ (‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
SAVE_DIR = "detected_waste"          # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏†‡∏≤‡∏û

# ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏™‡∏µ‡∏¢‡∏á
last_detected_class_for_speech = -1
current_streak_class = -1
current_streak_length = 0
last_announced_class = -1
last_announced_time = 0.0

# ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏†‡∏≤‡∏û
last_saved_class = -1
last_saved_time = 0.0

def save_detected_image(frame, class_id, confidence):
    """
    ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏¢‡∏∞
    """
    global last_saved_class, last_saved_time
    
    if not SAVE_IMAGES:
        return
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö cooldown
    now = time.time()
    if class_id == last_saved_class and (now - last_saved_time) < SAVE_COOLDOWN_SECONDS:
        return
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö confidence threshold
    if confidence < SAVE_CONF_THRESHOLD:
        return
    
    try:
        # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™
        class_name = CLASS_NAME_MAP.get(class_id, f"unknown_{class_id}")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏¢‡∏∞
        class_dir = Path(SAVE_DIR) / class_name
        class_dir.mkdir(parents=True, exist_ok=True)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå: timestamp_class_conf.jpg
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]  # milliseconds
        filename = f"{timestamp}_{class_name}_{confidence:.2f}.jpg"
        filepath = class_dir / filename
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏†‡∏≤‡∏û
        cv2.imwrite(str(filepath), frame)
        
        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
        last_saved_class = class_id
        last_saved_time = now
        
        print(f"[SAVE] Saved: {filepath}")
        
    except Exception as e:
        print(f"[SAVE] Error saving image: {e}")

def run_speech_in_background():
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏£‡∏±‡∏ô‡πÉ‡∏ô Thread ‡πÅ‡∏¢‡∏Å
    ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ speak_guidance ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏Ñ‡πâ‡∏≤‡∏á
    """
    global last_detected_class_for_speech
    while True:
        if last_detected_class_for_speech != -1:
            # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡πà‡∏≤‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
            class_to_speak = last_detected_class_for_speech
            last_detected_class_for_speech = -1
            
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏û‡∏π‡∏î (‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏µ Debounce ‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á)
            speak_guidance(class_to_speak)
        # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô busy loop
        time.sleep(0.05)

def process_frame(frame):
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà Gradio ‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡πÄ‡∏ü‡∏£‡∏°‡∏à‡∏≤‡∏Å Webcam
    """
    global last_detected_class_for_speech, current_streak_class, current_streak_length
    global last_announced_class, last_announced_time
    
    # 1. ‡∏û‡∏•‡∏¥‡∏Å‡πÄ‡∏ü‡∏£‡∏° (‡∏Å‡∏•‡πâ‡∏≠‡∏á Webcam ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏Å‡∏•‡∏±‡∏ö‡∏î‡πâ‡∏≤‡∏ô)
    frame = cv2.flip(frame, 1)
    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô BGR ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• (Gradio ‡∏õ‡πâ‡∏≠‡∏ô RGB)
    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
    
    # 2. ‡∏™‡∏±‡πà‡∏á‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡πÉ‡∏ô‡πÄ‡∏ü‡∏£‡∏°
    results = model(frame_bgr, conf=0.25, imgsz=640, verbose=False, max_det=300)
    
    # 3. ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™‡∏•‡∏á‡∏ö‡∏ô‡∏†‡∏≤‡∏û (‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô .plot() ‡∏Ç‡∏≠‡∏á ultralytics)
    annotated_bgr = results[0].plot()
    annotated_frame = cv2.cvtColor(annotated_bgr, cv2.COLOR_BGR2RGB)
    try:
        print("boxes:", len(results[0].boxes))
    except Exception:
        pass
    
    # 4. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏à‡∏≠‡∏≠‡∏∞‡πÑ‡∏£‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    boxes = results[0].boxes
    if boxes is not None and len(boxes) > 0:
        detected_class_tensor = boxes.cls[0]
        detected_class = int(detected_class_tensor.item())
        detected_conf = float(boxes.conf[0].item()) if boxes.conf is not None else 0.0

        if detected_conf >= SPEECH_CONF_THRESHOLD:
            if detected_class == current_streak_class:
                current_streak_length += 1
            else:
                current_streak_class = detected_class
                current_streak_length = 1

            now = time.time()
            should_trigger_speech = (
                current_streak_length >= SUSTAINED_FRAME_THRESHOLD and
                (
                    detected_class != last_announced_class or
                    now - last_announced_time >= ANNOUNCE_COOLDOWN_SECONDS
                )
            )

            if should_trigger_speech:
                # ‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÉ‡∏´‡πâ‡∏û‡∏π‡∏î (‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏à‡∏∞‡∏û‡∏π‡∏î‡∏à‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏°‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ detection ‡∏ï‡πà‡∏≠)
                last_detected_class_for_speech = detected_class
                last_announced_class = detected_class
                last_announced_time = now
                
                # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ (‡πÉ‡∏ä‡πâ annotated_bgr ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏£‡∏≠‡∏ö‡πÅ‡∏•‡πâ‡∏ß)
                save_detected_image(annotated_bgr, detected_class, detected_conf)
                
                try:
                    print(f"[SPEECH] trigger class={detected_class} conf={detected_conf:.2f}")
                except Exception:
                    pass
        else:
            # conf ‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ - reset streak ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏û‡∏π‡∏î‡∏≠‡∏¢‡∏π‡πà
            current_streak_class = -1
            current_streak_length = 0
    else:
        # ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏ - reset streak ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏û‡∏π‡∏î‡∏≠‡∏¢‡∏π‡πà
        # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ queue ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∞‡∏û‡∏π‡∏î‡∏à‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏°‡∏µ detection ‡∏ï‡πà‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        current_streak_class = -1
        current_streak_length = 0

    # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏£‡∏≠‡∏ö‡∏ß‡∏≤‡∏î‡πÅ‡∏•‡πâ‡∏ß ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö
    return annotated_frame

# --- ‡∏™‡∏£‡πâ‡∏≤‡∏á Gradio Interface ---
def main():
    print("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Gradio Interface...")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏†‡∏≤‡∏û
    if SAVE_IMAGES:
        save_path = Path(SAVE_DIR)
        save_path.mkdir(exist_ok=True)
        print(f"‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏†‡∏≤‡∏û‡πÑ‡∏õ‡∏ó‡∏µ‡πà: {save_path.absolute()}")
    
    # ‡πÄ‡∏£‡∏¥‡πà‡∏° Thread ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏û‡∏π‡∏î‡πÅ‡∏¢‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏´‡∏≤‡∏Å
    speech_thread = threading.Thread(target=run_speech_in_background, daemon=True)
    speech_thread.start()

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö‡∏î‡πâ‡∏ß‡∏¢ Blocks ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏° UI ‡πÑ‡∏î‡πâ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
    with gr.Blocks(title="‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏±‡∏î‡πÅ‡∏¢‡∏Å‡∏Ç‡∏¢‡∏∞‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ü§ñ ‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏±‡∏î‡πÅ‡∏¢‡∏Å‡∏Ç‡∏¢‡∏∞‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞ (AI Waste Sorter)")
        gr.Markdown("‡πÇ‡∏Ñ‡∏£‡∏á‡∏á‡∏≤‡∏ô‡πÇ‡∏î‡∏¢: ‡∏û‡∏á‡∏®‡∏†‡∏±‡∏Ñ, ‡∏Å‡∏§‡∏ï‡∏¥‡∏ô, ‡∏†‡∏π‡∏£‡∏¥‡∏ä‡∏ó‡∏±‡∏ï (‡πÉ‡∏ä‡πâ YOLOv12)")
        
        with gr.Row():
            input_image = gr.Image(
                type="numpy",
                sources=["webcam"],
                streaming=True,
                label="‡∏à‡πà‡∏≠‡∏Ç‡∏¢‡∏∞‡∏ó‡∏µ‡πà‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏ô‡∏µ‡πâ",
                show_label=True,
                show_download_button=False,
                show_share_button=False,
            )
            output_image = gr.Image(
                type="numpy",
                label="‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö",
                show_label=True,
                show_download_button=False,
                show_share_button=False,
            )
        
        # ‡πÉ‡∏ä‡πâ streaming event ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö real-time processing (‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏∏‡πà‡∏° Clear/Flag)
        input_image.stream(
            fn=process_frame,
            inputs=input_image,
            outputs=output_image,
        )
    
    # ‡∏£‡∏±‡∏ô‡πÅ‡∏≠‡∏õ
    print("Interface ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô. ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ô‡πÄ‡∏ö‡∏£‡∏≤‡∏ß‡πå‡πÄ‡∏ã‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì...")
    demo.launch(share=False)  # share=True ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÉ‡∏´‡πâ‡∏Ñ‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡∏î‡∏π

if __name__ == '__main__':
    main()