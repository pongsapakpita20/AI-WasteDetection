# ------------------------------------
# ‡πÑ‡∏ü‡∏•‡πå: app.py
# ------------------------------------
import gradio as gr
import cv2
from ultralytics import YOLO
from voice_guidance import speak_guidance  # Import ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏û‡∏π‡∏î
import threading
import time

# -------------------------------------------------------------------
# (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç!) ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Path ‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå best.pt ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏ó‡∏£‡∏ô‡πÑ‡∏î‡πâ
# -------------------------------------------------------------------
MODEL_PATH = 'runs/detect/yolo12m_final/weights/best.pt' # << ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
# -------------------------------------------------------------------

# 1. ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• AI ‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß
try:
    print(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏≤‡∏Å: {MODEL_PATH}")
    model = YOLO(MODEL_PATH)
    print("‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
except Exception as e:
    print(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•: {e}")
    print("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Path ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà")
    exit()

# ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏™‡∏µ‡∏¢‡∏á
SPEECH_CONF_THRESHOLD = 0.4          # conf ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏û‡∏π‡∏î (‡∏•‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏û‡∏π‡∏î‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô)
SUSTAINED_FRAME_THRESHOLD = 2        # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏ü‡∏£‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏Å‡πà‡∏≠‡∏ô‡∏û‡∏π‡∏î (‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î)
ANNOUNCE_COOLDOWN_SECONDS = 6        # ‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏π‡∏î‡∏ã‡πâ‡∏≥‡∏Ñ‡∏•‡∏≤‡∏™‡πÄ‡∏î‡∏¥‡∏°

# ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏™‡∏µ‡∏¢‡∏á
last_detected_class_for_speech = -1
current_streak_class = -1
current_streak_length = 0
last_announced_class = -1
last_announced_time = 0.0

def run_speech_in_background():
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏£‡∏±‡∏ô‡πÉ‡∏ô Thread ‡πÅ‡∏¢‡∏Å
    ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ speak_guidance ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏Ñ‡πâ‡∏≤‡∏á
    """
    global last_detected_class_for_speech
    while True:
        if last_detected_class_for_speech != -1:
            # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡πà‡∏≤‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
            class_to_speak = last_detected_class_for_speech
            last_detected_class_for_speech = -1
            
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏û‡∏π‡∏î (‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏µ Debounce ‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á)
            speak_guidance(class_to_speak)
        # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô busy loop
        time.sleep(0.05)

def process_frame(frame):
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà Gradio ‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡πÄ‡∏ü‡∏£‡∏°‡∏à‡∏≤‡∏Å Webcam
    """
    global last_detected_class_for_speech, current_streak_class, current_streak_length
    global last_announced_class, last_announced_time
    
    # 1. ‡∏û‡∏•‡∏¥‡∏Å‡πÄ‡∏ü‡∏£‡∏° (‡∏Å‡∏•‡πâ‡∏≠‡∏á Webcam ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏Å‡∏•‡∏±‡∏ö‡∏î‡πâ‡∏≤‡∏ô)
    frame = cv2.flip(frame, 1)
    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô BGR ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• (Gradio ‡∏õ‡πâ‡∏≠‡∏ô RGB)
    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
    
    # 2. ‡∏™‡∏±‡πà‡∏á‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡πÉ‡∏ô‡πÄ‡∏ü‡∏£‡∏°
    results = model(frame_bgr, conf=0.25, imgsz=640, verbose=False, max_det=300)
    
    # 3. ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™‡∏•‡∏á‡∏ö‡∏ô‡∏†‡∏≤‡∏û (‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô .plot() ‡∏Ç‡∏≠‡∏á ultralytics)
    annotated_bgr = results[0].plot()
    annotated_frame = cv2.cvtColor(annotated_bgr, cv2.COLOR_BGR2RGB)
    try:
        print("boxes:", len(results[0].boxes))
    except Exception:
        pass
    
    # 4. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏à‡∏≠‡∏≠‡∏∞‡πÑ‡∏£‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    boxes = results[0].boxes
    if boxes is not None and len(boxes) > 0:
        detected_class_tensor = boxes.cls[0]
        detected_class = int(detected_class_tensor.item())
        detected_conf = float(boxes.conf[0].item()) if boxes.conf is not None else 0.0

        if detected_conf >= SPEECH_CONF_THRESHOLD:
            if detected_class == current_streak_class:
                current_streak_length += 1
            else:
                current_streak_class = detected_class
                current_streak_length = 1

            now = time.time()
            should_trigger_speech = (
                current_streak_length >= SUSTAINED_FRAME_THRESHOLD and
                (
                    detected_class != last_announced_class or
                    now - last_announced_time >= ANNOUNCE_COOLDOWN_SECONDS
                )
            )

            if should_trigger_speech:
                # ‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÉ‡∏´‡πâ‡∏û‡∏π‡∏î (‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏à‡∏∞‡∏û‡∏π‡∏î‡∏à‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏°‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ detection ‡∏ï‡πà‡∏≠)
                last_detected_class_for_speech = detected_class
                last_announced_class = detected_class
                last_announced_time = now
                try:
                    print(f"[SPEECH] trigger class={detected_class} conf={detected_conf:.2f}")
                except Exception:
                    pass
        else:
            # conf ‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ - reset streak ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏û‡∏π‡∏î‡∏≠‡∏¢‡∏π‡πà
            current_streak_class = -1
            current_streak_length = 0
    else:
        # ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏ - reset streak ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏û‡∏π‡∏î‡∏≠‡∏¢‡∏π‡πà
        # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ queue ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∞‡∏û‡∏π‡∏î‡∏à‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏°‡∏µ detection ‡∏ï‡πà‡∏≠‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        current_streak_class = -1
        current_streak_length = 0

    # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏£‡∏≠‡∏ö‡∏ß‡∏≤‡∏î‡πÅ‡∏•‡πâ‡∏ß ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö
    return annotated_frame

# --- ‡∏™‡∏£‡πâ‡∏≤‡∏á Gradio Interface ---
def main():
    print("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á Gradio Interface...")
    
    # ‡πÄ‡∏£‡∏¥‡πà‡∏° Thread ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏û‡∏π‡∏î‡πÅ‡∏¢‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏´‡∏≤‡∏Å
    speech_thread = threading.Thread(target=run_speech_in_background, daemon=True)
    speech_thread.start()

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö
    iface = gr.Interface(
        fn=process_frame,
        inputs=gr.Image(
            type="numpy", 
            sources=["webcam"],
            streaming=True,
            label="‡∏à‡πà‡∏≠‡∏Ç‡∏¢‡∏∞‡∏ó‡∏µ‡πà‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏ô‡∏µ‡πâ"
        ),
        outputs=gr.Image(
            type="numpy", 
            label="‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö"
        ),
        live=True, # ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Real-time
        title="ü§ñ ‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡∏±‡∏î‡πÅ‡∏¢‡∏Å‡∏Ç‡∏¢‡∏∞‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞ (AI Waste Sorter)",
        description="‡πÇ‡∏Ñ‡∏£‡∏á‡∏á‡∏≤‡∏ô‡πÇ‡∏î‡∏¢: ‡∏û‡∏á‡∏®‡∏†‡∏±‡∏Ñ, ‡∏Å‡∏§‡∏ï‡∏¥‡∏ô, ‡∏†‡∏π‡∏£‡∏¥‡∏ä‡∏ó‡∏±‡∏ï (‡πÉ‡∏ä‡πâ YOLOv12)"
    )
    
    # 8. ‡∏£‡∏±‡∏ô‡πÅ‡∏≠‡∏õ
    print("Interface ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô. ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ô‡πÄ‡∏ö‡∏£‡∏≤‡∏ß‡πå‡πÄ‡∏ã‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì...")
    iface.launch(share=False) # share=True ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå‡πÉ‡∏´‡πâ‡∏Ñ‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡∏î‡∏π

if __name__ == '__main__':
    main()